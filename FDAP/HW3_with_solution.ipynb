{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d36b06",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "Student Name: xxx\n",
    "\n",
    "***\n",
    "### Task 1. Web Crawler\n",
    "\n",
    "Get the faculty list of Harvard Business School. (https://www.hbs.edu/faculty/Pages/browse.aspx?faculty=Current)\n",
    "\n",
    "The output should be the same as <code>hbs_faculty.csv</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2694d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.hbs.edu/faculty/Pages/browse.aspx?faculty=Current\")\n",
    "#page = requests.get(\"https://www.hbs.edu/faculty/Pages/browse.aspx?faculty=Current&unit=Finance\")\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "faculty = soup.find_all(class_='epsilon')[2:]\n",
    "\n",
    "names = [a.get_text() for a in faculty]\n",
    "websites = ['https://www.hbs.edu' + a['href'] for a in faculty]\n",
    "\n",
    "df = pd.DataFrame({\"Name\": names, \"Website\": websites})\n",
    "\n",
    "df.to_csv('hbs_faculty.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2e0de",
   "metadata": {},
   "source": [
    "***\n",
    "### Side mission\n",
    "***\n",
    "Mission 1. Get <code>full bio</code> of <mark>Rawi E. Abdelal</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.hbs.edu/faculty/Pages/profile.aspx?facId=6628\")\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "fullbio = soup.find(\"div\", {\"class\":\"fullbio\"})\n",
    "\n",
    "if fullbio:\n",
    "    fullbio_text = ''\n",
    "    for p in fullbio.find_all('p'):\n",
    "        #print(p.text)\n",
    "        fullbio_text = fullbio_text + p.text + '\\n'\n",
    "\n",
    "print(fullbio_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656140e",
   "metadata": {},
   "source": [
    "***\n",
    "Mission 2. Get <code>title</code>, <code>unit</code>, and <code>phone number</code> of <mark>Rawi E. Abdelal</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72921eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "page = requests.get(\"https://www.hbs.edu/faculty/Pages/profile.aspx?facId=6628\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "title = soup.find('h2')\n",
    "unit = soup.find(href=\"/faculty/units/bgie/Pages/default.aspx\")\n",
    "#unit = soup.find(href=re.compile(\"/faculty/units/bgie/Pages/default.aspx\"))\n",
    "phone = soup.find(class_='mu-uc')\n",
    "\n",
    "print(title.text)\n",
    "print(unit.text)\n",
    "print(phone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae61b7",
   "metadata": {},
   "source": [
    "***\n",
    "Mission 3. Append <code>title</code>, <code>unit</code>, and <code>phone number</code> of each professor to <code>hbs_faculty.csv</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46308bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hbs_faculty.csv')\n",
    "\n",
    "for i in df.index[:10]:\n",
    "    name = df['Name'][i]\n",
    "    website = df['Website'][i]\n",
    "    print('Crawling the info of %s:%s' % (i, name))\n",
    "    \n",
    "    page = requests.get(website)    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h2')\n",
    "    #unit = soup.find(href=re.compile(\"/faculty/units/bgie/Pages/default.aspx\"))\n",
    "    unit = soup.find(href=re.compile(\"/faculty/units/(.*?)/Pages/default.aspx\"))\n",
    "    phone = soup.find(class_='mu-uc')\n",
    "\n",
    "    if title:\n",
    "        df.loc[i, 'Title'] = title.text\n",
    "        #df.loc[i, 'Title_with_separator'] = title.get_text(separator=\"; \")\n",
    "\n",
    "    if unit:\n",
    "        df.loc[i, 'Unit'] = unit.text\n",
    "    \n",
    "    if phone:\n",
    "        df.loc[i, 'Phone'] = phone.text\n",
    "\n",
    "df.to_csv('hbs_faculty_title.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea3b67",
   "metadata": {},
   "source": [
    "***\n",
    "Mission 4. Append <code>AREAS OF INTEREST</code> of <mark>Rawi E. Abdelal</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d83ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.hbs.edu/faculty/Pages/profile.aspx?facId=6628\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "areas = soup.find_all(href=re.compile(\"/faculty/Pages/browse.aspx\\?topic=\"))\n",
    "\n",
    "area_list = []\n",
    "for area in areas:\n",
    "    area_list.append(area.text)\n",
    "\n",
    "area_list_unique_sorted = sorted(set(area_list))\n",
    "area_text = '; '.join(area_list_unique_sorted)\n",
    "\n",
    "print(area_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f21b4d",
   "metadata": {},
   "source": [
    "***\n",
    "Mission 5. Append <code>AREAS OF INTEREST</code> and <code>ADDITIONAL TOPICS</code> of each professor to <code>hbs_faculty_title.csv</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f436b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hbs_faculty_title.csv')\n",
    "\n",
    "for i in df.index[:10]:\n",
    "    name = df['Name'][i]\n",
    "    website = df['Website'][i]\n",
    "    print('Crawling the info of %s:%s' % (i, name))\n",
    "    \n",
    "    page = requests.get(website)    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    areas = soup.find_all(href=re.compile(\"/faculty/Pages/browse.aspx\\?topic=\"))\n",
    "\n",
    "    if areas:\n",
    "        area_list = []\n",
    "        for area in areas:\n",
    "            area_list.append(area.text)\n",
    "\n",
    "        area_list_unique_sorted = sorted(set(area_list))\n",
    "        df.loc[i, 'Areas'] = '; '.join(area_list_unique_sorted)\n",
    "\n",
    "df.to_csv('hbs_faculty_area.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
